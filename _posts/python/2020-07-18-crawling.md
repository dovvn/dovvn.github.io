---
title:  "[파이썬 웹크롤링] 1. 기본 개념"
excerpt: "파이썬을 이용한 웹크롤링을 이해하고 주요 개념 정리"
permalink: /categories/python/web-crawling-01
author_profile: true
categories:
  - Python
tags:
  - Python
  - Crawling 
toc: true
toc_label: "목차"
toc_icon: "bookmark"
last_modified_at: 2020-07-18
---

{{page.excerpt}}  

# 공부 계획  
> 목표기간: 2020년 07월 18일 ~ 7월 31일(프로젝트 기간 포함)  
파이썬 웹크롤링을 공부하고 여러 소규모 프로젝트를 만든 후, 포폴용 플젝을 만드는것을 목표로 한다.
=> 프로젝트는 8월 까지 꼭 만들기! (하반기 시작 전)  <br/>

# 웹크롤링(Web Creawling)이란?
웹에서 데이터를 가져오는 행위   
이때, 웹에서 가져온 데이터를 내가 원하는 대로 가공하는 것을 **파싱(Parsing)**이라 한다.
예를 들어, 웹사이트는 HTML 형식으로 작성되었으므로 이를 가져와 보기에 편한 방식(ex.json)로 바꾸어 주는 것이 바로 파싱이다.  
**스크래핑(Scraping)**이란 데이터를 수집하는 모든 과정으로, 크롤링은 스크래핑의 방법 중 하나이다.  
=> 이를 정리하면, 웹크롤링을 한다, 파싱한다. == **웹상의 수많은 자료를 스크래핑(크롤링)하여 수집된 데이터를 파싱(가공)해서 내가 원하는 데이터를 추출하는 것**  <br/>

# 크롤링에 파이썬을 주로 사용하는 이유
* 웹크롤링에 필요한 라이브러리인  fake_useragent, BeautifulSoup, urllib, requests등이 있다.
* 파이썬에는 데이터 수집, 처리, 분석에 유용한 Pandas, TensorFlow(머신러닝) 패키지가 있다.  <br/>

# 크롤링을 하기 전 알아야하는 지식
## HTTP(Hyper Text Transfer Protocol)
인터넷상에서 데이터를 주고 받을 때 지켜야 하는 통신 규약
= 인터넷상에서 데이터를 주고 받을 수 있는 프로토콜(=규칙)  
=> HTTP통신을 통해 인터넷상에서 프로그램이 서로 정보를 주고 받을 수 있다.  
사용자가 url에 접속하게 되면 웹브라우저는 해당 페이지의 자원을 가지고 있는 웹서버에 요청(Request)하게 된다.
그럼, 웹서버는 해당 하는 파일을 찾아 다시 웹브라우저로 응답(Response)한다.

### HTTP Requset 확인 방법
크롬에서 개발자모드(f12) → Network탭 → 아무거나 눌러서 Headers확인  


# 파이썬 라이브러리 - 1)Requests
Python에서는 기본 라이브러리로 urllib가 제공되지만, Requests를 사용하면 좀 더 간결하게 HTTP요청을 할 수 있다.


# Reference
* [Zerocho Blog-HTTP란 무엇인가](https://www.zerocho.com/category/HTTP/post/5b344f3af94472001b17f2da)
* [Y.LAB - BeautifulSoup4를 이용한 파이썬 크롤링](https://yamalab.tistory.com/64)